{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a more explorative analysis on if meal patterns, in particular, accuracy of actions inside a meal, have difference between control mice and CASK-KD mice. \n",
    "\n",
    "Based on the results in this notebook, we would think there is no obvious difference between two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "import torch\n",
    "\n",
    "import meals as ml\n",
    "from meal_classifiers import *\n",
    "from unsupervised_helpers import *\n",
    "import numpy as np\n",
    "from preprocessing import read_excel_by_sheet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import wasserstein_distance, entropy\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "time_threshold = 60\n",
    "pellet_count_threshold = 2\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_meal_by_n_pellets(predictions, categories):\n",
    "    category_counts = {3: {'total': 0, 'zeros': 0},\n",
    "                    4: {'total': 0, 'zeros': 0},\n",
    "                    5: {'total': 0, 'zeros': 0}}\n",
    "\n",
    "    # Loop through the predictions and categories\n",
    "    for pred, cat in zip(predictions, categories):\n",
    "        if cat in category_counts:\n",
    "            category_counts[cat]['total'] += 1\n",
    "            if pred == 0:\n",
    "                category_counts[cat]['zeros'] += 1\n",
    "\n",
    "    temp_list = []\n",
    "    for cat in category_counts:\n",
    "        total = category_counts[cat]['total']\n",
    "        zeros = category_counts[cat]['zeros']\n",
    "        percentage = (zeros / total) * 100 if total > 0 else 0\n",
    "        temp_list.append(percentage)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = CNNClassifier(num_classes=2, maxlen=4).to(device)\n",
    "model = RNNClassifier(input_size=1, hidden_size=400, num_classes=2, num_layers=2).to(device)\n",
    "model.load_state_dict(torch.load('../data/LSTM_from_CASK.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_data = []\n",
    "cask_data = []\n",
    "\n",
    "for sheet in rev_ctrl_sheets[:]:\n",
    "    data = read_excel_by_sheet(sheet, rev_ctrl_path)\n",
    "    meals, meals_len = ml.extract_meals_for_model(data, time_threshold, pellet_count_threshold)\n",
    "    preds = predict(model, meals)\n",
    "    ctrl_data.append(good_meal_by_n_pellets(preds, meals_len))\n",
    "\n",
    "for sheet in rev_cask_sheets:\n",
    "    data = read_excel_by_sheet(sheet, rev_cask_path)\n",
    "    meals, meals_len = ml.extract_meals_for_model(data, time_threshold, pellet_count_threshold)\n",
    "    preds = predict(model, meals)\n",
    "    cask_data.append(good_meal_by_n_pellets(preds, meals_len))\n",
    "\n",
    "ctrl_data = np.array(ctrl_data)\n",
    "cask_data = np.array(cask_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(ctrl_data, cask_data)\n",
    "\n",
    "mean_similarity = np.mean(similarity_matrix)\n",
    "max_similarity = np.max(similarity_matrix)\n",
    "min_similarity = np.min(similarity_matrix)\n",
    "variance_similarity = np.var(similarity_matrix)\n",
    "\n",
    "print(\"Mean Similarity:\", mean_similarity)\n",
    "print(\"Max Similarity:\", max_similarity)\n",
    "print(\"Min Similarity:\", min_similarity)\n",
    "print(\"Variance of Similarity:\", variance_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroids Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centroids\n",
    "centroid_M1 = np.mean(ctrl_data, axis=0)\n",
    "centroid_M2 = np.mean(cask_data, axis=0)\n",
    "\n",
    "# Compute cosine similarity between centroids\n",
    "centroid_similarity = cosine_similarity([centroid_M1], [centroid_M2])[0][0]\n",
    "\n",
    "print(\"Cosine Similarity Between Group Centroids:\", centroid_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_M1 = np.std(ctrl_data, axis=0)\n",
    "std_M2 = np.std(cask_data, axis=0)\n",
    "print(f'Control Group Mean: {centroid_M1}; STD: {std_M1}')\n",
    "print(f'CASK Group Mean: {centroid_M2}; STD: {std_M2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise Wasserstein distance for each dimension\n",
    "# Combine data for normalization\n",
    "combined_data = np.vstack([ctrl_data, cask_data])\n",
    "\n",
    "# Compute mean and std for normalization (column-wise)\n",
    "mean = np.mean(combined_data, axis=0)\n",
    "std = np.std(combined_data, axis=0)\n",
    "\n",
    "# Normalize both datasets\n",
    "ctrl_data_normalized = (ctrl_data - mean) / std\n",
    "cask_data_normalized = (cask_data - mean) / std\n",
    "\n",
    "# Compute Wasserstein distance for normalized data (dimension-wise)\n",
    "wasserstein_distances = [\n",
    "    wasserstein_distance(ctrl_data_normalized[:, i], cask_data_normalized[:, i])\n",
    "    for i in range(ctrl_data_normalized.shape[1])\n",
    "]\n",
    "unnormalized_distance = [wasserstein_distance(ctrl_data[:, i], cask_data[:, i])\n",
    "    for i in range(ctrl_data.shape[1])]\n",
    "unnormalized_mean = np.mean(unnormalized_distance)\n",
    "\n",
    "mean_wasserstein_distance = np.mean(wasserstein_distances)\n",
    "\n",
    "print(\"Wasserstein distances per dimension (normalized):\", wasserstein_distances)\n",
    "print(\"Mean Wasserstein Distance (normalized):\", mean_wasserstein_distance)\n",
    "print(\"Mean Wasserstein distance (unnormalized):\", unnormalized_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten matrices to compare all dimensions together\n",
    "M1_flattened = ctrl_data.flatten()\n",
    "M2_flattened = cask_data.flatten()\n",
    "\n",
    "# Estimate probability distributions with histograms\n",
    "bins = np.histogram_bin_edges(np.concatenate([M1_flattened, M2_flattened]), bins=30)\n",
    "P, _ = np.histogram(M1_flattened, bins=bins, density=True)\n",
    "Q, _ = np.histogram(M2_flattened, bins=bins, density=True)\n",
    "\n",
    "# Add a small value to avoid division by zero\n",
    "epsilon = 1e-10\n",
    "P += epsilon\n",
    "Q += epsilon\n",
    "\n",
    "# Normalize to make valid probability distributions\n",
    "P /= P.sum()\n",
    "Q /= Q.sum()\n",
    "\n",
    "# Compute KL divergence\n",
    "kl_divergence = entropy(P, Q)  # D_KL(P || Q)\n",
    "print(f\"KL Divergence (ctrl || cask): {kl_divergence}\")\n",
    "kl_divergence = entropy(Q, P)  # D_KL(P || Q)\n",
    "print(f\"KL Divergence (cask || ctrl): {kl_divergence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add control group points\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=ctrl_data[:, 0],\n",
    "    y=ctrl_data[:, 1],\n",
    "    z=ctrl_data[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='blue', opacity=0.7),\n",
    "    name='Control Group'\n",
    "))\n",
    "\n",
    "# Add cask group points\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=cask_data[:, 0],\n",
    "    y=cask_data[:, 1],\n",
    "    z=cask_data[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='red', opacity=0.7),\n",
    "    name='Cask Group'\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"3D Visualization of Control and Cask Groups (Unnormalized)\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Feature 1', backgroundcolor='rgba(211,211,211)'),\n",
    "        yaxis=dict(title='Feature 2', backgroundcolor='rgba(211,211,211)'),\n",
    "        zaxis=dict(title='Feature 3', backgroundcolor='rgba(211,211,211)')\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),  # Reduce whitespace\n",
    "    width=800,  # Adjust width of the plot\n",
    "    height=600  # Adjust height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot (interactive in Jupyter)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add control group points\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=ctrl_data_normalized[:, 0],\n",
    "    y=ctrl_data_normalized[:, 1],\n",
    "    z=ctrl_data_normalized[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='blue', opacity=0.7),\n",
    "    name='Control Group'\n",
    "))\n",
    "\n",
    "# Add cask group points\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=cask_data_normalized[:, 0],\n",
    "    y=cask_data_normalized[:, 1],\n",
    "    z=cask_data_normalized[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='red', opacity=0.7),\n",
    "    name='Cask Group'\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"3D Visualization of Control and Cask Groups (Normalized)\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Feature 1', backgroundcolor='rgba(211,211,211)'),\n",
    "        yaxis=dict(title='Feature 2', backgroundcolor='rgba(211,211,211)'),\n",
    "        zaxis=dict(title='Feature 3', backgroundcolor='rgba(211,211,211)')\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),  # Reduce whitespace\n",
    "    width=800,  # Adjust width of the plot\n",
    "    height=600  # Adjust height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot (interactive in Jupyter)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([ctrl_data, cask_data])\n",
    "y = np.hstack([np.zeros(ctrl_data.shape[0]), np.ones(cask_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Compute log loss (cross-entropy loss)\n",
    "loss = log_loss(y_test, y_pred_proba)\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=model.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=model.predict(X_test))\n",
    "print(f\"Log Loss: {loss:.4f}; Train Accuracy: {train_acc:.3f}; Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Align predicted labels with true labels (clusters are arbitrarily labeled)\n",
    "if accuracy_score(y, y_pred) < accuracy_score(y, 1 - y_pred):\n",
    "    y_pred = 1 - y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Clustering Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
